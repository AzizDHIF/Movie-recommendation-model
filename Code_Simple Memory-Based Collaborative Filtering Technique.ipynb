{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAqimb9TSNCq"
   },
   "source": [
    "## Simple Memory-Based Collaborative Filtering Technique\n",
    "\n",
    "### Overview\n",
    "\n",
    "The below code details our (after-the-fact) efforts to build a memory-based collaborative filtering (CF) recommendation system for the Santander Product Recommendation challenge, which ran a couple of years ago. The top submissions were based on classic Kaggle-winning algorithms like random forests and boosting algorithms, but we wanted to build something more in-line with the recommendation systems literature. The below can thus be viewed as a basic how-to on building a recommendation engine on difficultly-structured data; the main difficulties in this dataset arise from the fact that product information is sparse (in a traditional CF problem, we have ratings or purchase volumes attached to each product, rather than the binary data format we are faced with here).\n",
    "\n",
    "A good introduction to CF can be found at https://towardsdatascience.com/various-implementations-of-collaborative-filtering-100385c6dfe0\n",
    "\n",
    "The presence of demographic data also posed a challenge: what is the best way to incorporate these informative variables in the CF model? We loosely followed the approach laid out in this paper: https://pdfs.semanticscholar.org/a621/441e7b688580707af3a4bf0ebff8c9e3d640.pdf\n",
    "\n",
    "We approached the problem in 8 main steps:\n",
    "\n",
    "1. ** Data cleaning: **\n",
    "this took the form of dropping columns deemed surplus to requirements, imputing missing data.\n",
    "2. ** Feature engineering:**\n",
    "binning continuous data into factor variables and mutating the product ownership variables to give an indication of purchase and ownership in the previous month.\n",
    "3. ** Data subsetting: **\n",
    "Based on other kernels and our own investigations, we only used June 2015 data (i.e. one year before the test data) as a predictor.\n",
    "4. ** De-duplicate the data: **\n",
    "there are >900,000 test individuals, but most of these will have identical purchase histories given the data sparsity: we thus de-duplicate based on the variables that will be considered for CF, and create an index to map results back to the unique users. This de-duplication is employed in all the major algorithms we used, and substantially sped up processing time.\n",
    "5. ** Build a demographic-based and memory-based similarity matrix: **\n",
    "because of the large data size, we calculated the Manhattan distance of each test row to the training rows and used the inverse distance to derive similarities/purchase probabilities...\n",
    "6. ** ...combine demographic-based and memory-based probabilities: **\n",
    "we combine the two models using a range of candidate weights, and nullified the probability of products each user already owns...\n",
    "7. ** ...and derive recommendations: **\n",
    "using the derived probabilities, we output the top seven recommendations. We iterate steps 5-7 over each unique test user profile, and ascertain the optimal demographic/memory mixing parameter.\n",
    "8. ** Re-run model using all training data for optimal mixing parameter **\n",
    "\n",
    "Of course this solution is not exhaustive, and steps for algorithmic improvement are outlined at the end of the Kernel. We hope to be able to implement these improvements in-kernel at a future date!\n",
    "\n",
    "Special thanks to George Hartshorn for sharing the data-cleaning code this notebook is based on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WseS2KEzSNCy",
    "outputId": "ab72c76a-6bbd-42f3-bdac-a0e7253353d9"
   },
   "outputs": [],
   "source": [
    "#Import modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "from operator import sub\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing, ensemble, metrics\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import math\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "# Correction : 'wminkowski' n'existe plus dans scipy.spatial.distance ‚Üí supprim√© de l'import\n",
    "# Si une distance de Minkowski pond√©r√©e est n√©cessaire, elle devra √™tre impl√©ment√©e manuellement\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Check data library\n",
    "\n",
    "#import os\n",
    "# print(os.listdir(\"../input\"))  # <- d√©sactiv√© car ne s'applique pas en local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "xvY3McHRSNC0"
   },
   "outputs": [],
   "source": [
    "#Import data\n",
    "path = 'C:/Users/ghass/Downloads/projet/'\n",
    "#Le DtypeWarning se produit parce que pandas d√©tecte des types de donn√©es mixtes dans les colonnes sp√©cifi√©es (5, 8, 11, 15 pour traindat et 15 pour testdat) lors de la lecture des fichiers CSV.\n",
    "#Par cons√©quent, le fait de d√©finir low_memory=False force pandas √† charger l'int√©gralit√© du fichier en m√©moire, ce qui lui permet de d√©duire les types de donn√©es de mani√®re plus pr√©cise, et ainsi de supprimer cet avertissement.\n",
    "traindat = pd.read_csv(path + 'train_ver2.csv', low_memory = False)\n",
    "testdat = pd.read_csv(path + 'test_ver2.csv', low_memory = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hv0CFUhqSNC1"
   },
   "source": [
    "## Step 1: Data Cleaning\n",
    "Now that the data is read in, we can get going with everyone's favourite bit: ** data cleaning! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "iVeEC4-XSNC3"
   },
   "outputs": [],
   "source": [
    "#Define columns of interest, based on other kernels' output\n",
    "\n",
    "demographic_cols = ['fecha_dato',\n",
    " 'ncodpers','ind_empleado','pais_residencia','sexo','age','fecha_alta','ind_nuevo','antiguedad','indrel',\n",
    " 'indrel_1mes','tiprel_1mes','indresi','indext','canal_entrada','indfall',\n",
    " 'tipodom','cod_prov','ind_actividad_cliente','renta','segmento']\n",
    "\n",
    "notuse = [\"ult_fec_cli_1t\",\"nomprov\"]\n",
    "\n",
    "product_col = [\n",
    " 'ind_ahor_fin_ult1','ind_aval_fin_ult1','ind_cco_fin_ult1','ind_cder_fin_ult1','ind_cno_fin_ult1','ind_ctju_fin_ult1',\n",
    " 'ind_ctma_fin_ult1','ind_ctop_fin_ult1','ind_ctpp_fin_ult1','ind_deco_fin_ult1','ind_deme_fin_ult1',\n",
    " 'ind_dela_fin_ult1','ind_ecue_fin_ult1','ind_fond_fin_ult1','ind_hip_fin_ult1','ind_plan_fin_ult1',\n",
    " 'ind_pres_fin_ult1','ind_reca_fin_ult1','ind_tjcr_fin_ult1','ind_valo_fin_ult1','ind_viv_fin_ult1','ind_nomina_ult1',\n",
    " 'ind_nom_pens_ult1','ind_recibo_ult1']\n",
    "\n",
    "train_cols = demographic_cols + product_col\n",
    "\n",
    "# Create trimmed datasets\n",
    "\n",
    "traindat = traindat.filter(train_cols)\n",
    "testdat  = testdat.filter(train_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "K58PXu7ZSNC3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato                     0\n",
       "ncodpers                       0\n",
       "ind_empleado               27734\n",
       "pais_residencia            27734\n",
       "sexo                       27804\n",
       "age                            0\n",
       "fecha_alta                 27734\n",
       "ind_nuevo                  27734\n",
       "antiguedad                     0\n",
       "indrel                     27734\n",
       "indrel_1mes               149781\n",
       "tiprel_1mes               149781\n",
       "indresi                    27734\n",
       "indext                     27734\n",
       "canal_entrada             186126\n",
       "indfall                    27734\n",
       "tipodom                    27735\n",
       "cod_prov                   93591\n",
       "ind_actividad_cliente      27734\n",
       "renta                    2794375\n",
       "segmento                  189368\n",
       "ind_ahor_fin_ult1              0\n",
       "ind_aval_fin_ult1              0\n",
       "ind_cco_fin_ult1               0\n",
       "ind_cder_fin_ult1              0\n",
       "ind_cno_fin_ult1               0\n",
       "ind_ctju_fin_ult1              0\n",
       "ind_ctma_fin_ult1              0\n",
       "ind_ctop_fin_ult1              0\n",
       "ind_ctpp_fin_ult1              0\n",
       "ind_deco_fin_ult1              0\n",
       "ind_deme_fin_ult1              0\n",
       "ind_dela_fin_ult1              0\n",
       "ind_ecue_fin_ult1              0\n",
       "ind_fond_fin_ult1              0\n",
       "ind_hip_fin_ult1               0\n",
       "ind_plan_fin_ult1              0\n",
       "ind_pres_fin_ult1              0\n",
       "ind_reca_fin_ult1              0\n",
       "ind_tjcr_fin_ult1              0\n",
       "ind_valo_fin_ult1              0\n",
       "ind_viv_fin_ult1               0\n",
       "ind_nomina_ult1            16063\n",
       "ind_nom_pens_ult1          16063\n",
       "ind_recibo_ult1                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identify columns with missing data\n",
    "\n",
    "traindat.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rKPoQgFSNC4"
   },
   "source": [
    "Missing value variables can be broken down into three categories:\n",
    "1. Missing data for factor variables: we either impute the most common factor level, as the missing variables are a small subset of the total data, or set to a new 'missing' level if this will imbalance the factor classes.\n",
    "2. Missing data for numerical variables: we can use a more granular imputation, as other kernels have, by setting the missing value equal to the average for each province.\n",
    "3. Missing data for product variables: these are NA as the customers are not eligible to purchase the product. Thus they don't have the product, and their ownership status can be set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "I8cYrJFISNC6"
   },
   "outputs": [],
   "source": [
    "#Impute training data\n",
    "\n",
    "traindat.age = pd.to_numeric(traindat.age, errors='coerce')\n",
    "traindat.renta = pd.to_numeric(traindat.renta, errors='coerce')\n",
    "traindat.antiguedad = pd.to_numeric(traindat.antiguedad, errors='coerce')\n",
    "\n",
    "traindat.loc[traindat['ind_empleado'].isnull(),'ind_empleado'] = 'N'\n",
    "traindat.loc[traindat['pais_residencia'].isnull(),'pais_residencia'] = 'ES'\n",
    "traindat.loc[traindat['sexo'].isnull(),'sexo'] = 'V'\n",
    "traindat.fecha_alta = traindat.fecha_alta.astype('datetime64[ns]')\n",
    "traindat.loc[traindat['fecha_alta'].isnull(), 'fecha_alta'] = pd.Timestamp(2011,9,1)\n",
    "traindat.loc[traindat['ind_nuevo'].isnull(), 'ind_nuevo'] = 0\n",
    "traindat.loc[traindat['indrel'].isnull(), 'indrel'] = 1\n",
    "traindat.indrel_1mes = traindat.indrel_1mes.astype('str').str.slice(0,1)\n",
    "traindat.loc[traindat['indrel_1mes'].isnull(), 'indrel_1mes'] = '1'\n",
    "traindat.loc[traindat['tiprel_1mes'].isnull(), 'tiprel_1mes'] = 'I'\n",
    "traindat.loc[traindat['indresi'].isnull(), 'indresi'] = 'S'\n",
    "traindat.loc[traindat['indext'].isnull(), 'indext'] = 'N'\n",
    "traindat.loc[traindat['canal_entrada'].isnull(), 'canal_entrada'] = 'MIS'\n",
    "traindat.loc[traindat['indfall'].isnull(), 'indfall'] = 'N'\n",
    "traindat.loc[traindat['tipodom'].isnull(), 'tipodom'] = 0.0\n",
    "traindat.loc[traindat['cod_prov'].isnull(), 'cod_prov'] = 28.0\n",
    "traindat.loc[traindat['ind_actividad_cliente'].isnull(), 'ind_actividad_cliente'] = 0.0\n",
    "traindat[\"renta\"] = traindat[['renta','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace renta with provincial mean\n",
    "traindat[\"age\"] = traindat[['age','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace age with provincial mean\n",
    "traindat[\"antiguedad\"] = traindat[['antiguedad','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace antiguedad with provincial mean\n",
    "traindat.loc[traindat['segmento'].isnull(), 'segmento'] = '02 - PARTICULARES'\n",
    "traindat.loc[traindat['ind_nomina_ult1'].isnull(), 'ind_nomina_ult1'] = 0\n",
    "traindat.loc[traindat['ind_nom_pens_ult1'].isnull(), 'ind_nom_pens_ult1'] = 0\n",
    "\n",
    "#Impute test data\n",
    "\n",
    "testdat.age = pd.to_numeric(testdat.age, errors='coerce')\n",
    "testdat.antiguedad = pd.to_numeric(testdat.antiguedad, errors='coerce')\n",
    "testdat.renta = pd.to_numeric(testdat.renta, errors='coerce')\n",
    "\n",
    "testdat.loc[testdat['sexo'].isnull(),'sexo'] = 'V'\n",
    "testdat.indrel_1mes = testdat.indrel_1mes.astype('str').str.slice(0,1)\n",
    "testdat.loc[testdat['indrel_1mes'].isnull(), 'indrel_1mes'] = '1'\n",
    "testdat.loc[testdat['tiprel_1mes'].isnull(), 'tiprel_1mes'] = 'I'\n",
    "testdat.loc[testdat['canal_entrada'].isnull(), 'canal_entrada'] = 'MIS'\n",
    "testdat.loc[testdat['cod_prov'].isnull(), 'cod_prov'] = 28.0\n",
    "testdat.loc[testdat['segmento'].isnull(), 'segmento'] = '02 - PARTICULARES'\n",
    "testdat[\"renta\"] = testdat[['renta','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace renta with provincial mean\n",
    "testdat[\"age\"] = testdat[['age','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace age with provincial mean\n",
    "testdat[\"antiguedad\"] = testdat[['antiguedad','cod_prov']].groupby(\"cod_prov\").transform(lambda x: x.fillna(x.mean())) #Replace antiguedad with provincial mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "oWBD8Mz0SNC7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fecha_dato               0\n",
       "ncodpers                 0\n",
       "ind_empleado             0\n",
       "pais_residencia          0\n",
       "sexo                     0\n",
       "age                      0\n",
       "fecha_alta               0\n",
       "ind_nuevo                0\n",
       "antiguedad               0\n",
       "indrel                   0\n",
       "indrel_1mes              0\n",
       "tiprel_1mes              0\n",
       "indresi                  0\n",
       "indext                   0\n",
       "canal_entrada            0\n",
       "indfall                  0\n",
       "tipodom                  0\n",
       "cod_prov                 0\n",
       "ind_actividad_cliente    0\n",
       "renta                    0\n",
       "segmento                 0\n",
       "ind_ahor_fin_ult1        0\n",
       "ind_aval_fin_ult1        0\n",
       "ind_cco_fin_ult1         0\n",
       "ind_cder_fin_ult1        0\n",
       "ind_cno_fin_ult1         0\n",
       "ind_ctju_fin_ult1        0\n",
       "ind_ctma_fin_ult1        0\n",
       "ind_ctop_fin_ult1        0\n",
       "ind_ctpp_fin_ult1        0\n",
       "ind_deco_fin_ult1        0\n",
       "ind_deme_fin_ult1        0\n",
       "ind_dela_fin_ult1        0\n",
       "ind_ecue_fin_ult1        0\n",
       "ind_fond_fin_ult1        0\n",
       "ind_hip_fin_ult1         0\n",
       "ind_plan_fin_ult1        0\n",
       "ind_pres_fin_ult1        0\n",
       "ind_reca_fin_ult1        0\n",
       "ind_tjcr_fin_ult1        0\n",
       "ind_valo_fin_ult1        0\n",
       "ind_viv_fin_ult1         0\n",
       "ind_nomina_ult1          0\n",
       "ind_nom_pens_ult1        0\n",
       "ind_recibo_ult1          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check to make sure all missing data has been filled\n",
    "traindat.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I02jA81HSNC8"
   },
   "source": [
    "## Step 2: Feature Engineering\n",
    "\n",
    "Feature engineering is one of the most important parts of good Kaggle performance, and in this competition it was vital. Our first step was to bin the continuous variables, so we could treat all variables as binary in the distance matrix, easily scaling demographic and ownership variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "T5JWBnN2SNC9"
   },
   "outputs": [],
   "source": [
    "# some more data cleaning\n",
    "\n",
    "#traindat[\"fecha_alta\"] = traindat[\"fecha_alta\"].astype(\"datetime64\")\n",
    "#testdat[\"fecha_alta\"] = testdat[\"fecha_alta\"].astype(\"datetime64\")\n",
    "#Utilisation \"datetime64\" sans sp√©cification de l‚Äôunit√© de temps, ce qui est obligatoire. Pandas attend une pr√©cision comme \"datetime64[ns]\" (nanosecondes), \"datetime64[D]\" (jour), etc.\n",
    "#Correction: \n",
    "traindat[\"fecha_alta\"] = pd.to_datetime(traindat[\"fecha_alta\"], errors='coerce')\n",
    "testdat[\"fecha_alta\"] = pd.to_datetime(testdat[\"fecha_alta\"], errors='coerce')\n",
    "# Observation: based on (omitted) EDA, a pre/post 2011 split would make sense for fecha_alta; as credit recovered following the 2008 crash, we may expect to see different user types\n",
    "\n",
    "# Observation: on a log scale, the salary data is broadly normal. We can take low-medium-high bounds using quartiles\n",
    "\n",
    "traindat[\"renta\"] = np.log(traindat[\"renta\"])\n",
    "testdat[\"renta\"] = np.log(testdat[\"renta\"])\n",
    "\n",
    "# bin the continuous variables\n",
    "\n",
    "bins_dt = pd.date_range('1994-01-01', freq='16Y', periods=3)\n",
    "bins_str = bins_dt.astype(str).values\n",
    "labels = ['({}, {}]'.format(bins_str[i-1], bins_str[i]) for i in range(1, len(bins_str))]\n",
    "\n",
    "traindat['fecha_alta'] = pd.cut(traindat.fecha_alta.astype(np.int64)//10**9,\n",
    "                   bins=bins_dt.astype(np.int64)//10**9,\n",
    "                   labels=labels)\n",
    "\n",
    "testdat['fecha_alta'] = pd.cut(testdat.fecha_alta.astype(np.int64)//10**9,\n",
    "                   bins=bins_dt.astype(np.int64)//10**9,\n",
    "                   labels=labels)\n",
    "\n",
    "\n",
    "bins_renta = [0,np.percentile(traindat.renta, 25),np.percentile(traindat.renta, 75),25]\n",
    "\n",
    "traindat['renta'] = pd.cut(traindat.renta,\n",
    "                   bins=bins_renta)\n",
    "\n",
    "testdat['renta'] = pd.cut(testdat.renta,\n",
    "                   bins=bins_renta)\n",
    "\n",
    "\n",
    "bins_age = [0,25,42,60,1000]\n",
    "labels_age = ['young','middle','older','old']\n",
    "\n",
    "traindat['age'] = pd.cut(traindat.age,\n",
    "                   bins=bins_age,\n",
    "                   labels=labels_age)\n",
    "\n",
    "testdat['age'] = pd.cut(testdat.age,\n",
    "                   bins=bins_age,\n",
    "                   labels=labels_age)\n",
    "\n",
    "\n",
    "bins_anti = [-1,220,300]\n",
    "labels_anti = ['new','old']\n",
    "\n",
    "#remove negative antiguedad values\n",
    "#traindat.antiguedad[traindat.antiguedad<0] = 0 desactiv√© et remplace pour assurer qu la modification la colonne antiguedad dans traindat directement et pas une copie.\n",
    "#Correction:\n",
    "traindat.loc[traindat[\"antiguedad\"] < 0, \"antiguedad\"] = 0\n",
    "\n",
    "\n",
    "\n",
    "traindat['antiguedad'] = pd.cut(traindat.antiguedad,\n",
    "                   bins=bins_anti,\n",
    "                   labels=labels_anti)\n",
    "\n",
    "testdat['antiguedad'] = pd.cut(testdat.antiguedad,\n",
    "                   bins=bins_anti,\n",
    "                   labels=labels_anti)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyL642ZMSNC-"
   },
   "source": [
    "## Step 3: Data Subsetting\n",
    "Jumping quickly ahead to ** step 3 **: now we have dealt with continuous variables, we can subset the data to only the months of interest. We use June 2015 (and its associated lagged month) in the final model, as well as the lagged May 2016 for the response variable. The data merge that follows breaks the 16GB kernel RAM limit if we don't bring the date subsetting forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "9IpIPxLnSNDB"
   },
   "outputs": [],
   "source": [
    "traindat = traindat[traindat.fecha_dato.isin(['2015-05-28','2015-06-28','2016-05-28'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obQ_i8tESNDB"
   },
   "source": [
    "This frees up space, allowing us to finish our feature engineering. The data as presented to us was based on ownership in each month, whereas we are interested in purchase in each month: our second step was to create two new features for each variable, which were ownership in the *previous* month and whether the product was purchased in each month. We used the latter as our response variables, and the former as our main explanatory variables to construct the distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "nFjTmYB9SNDB",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort completed\n",
      "merge completed\n"
     ]
    }
   ],
   "source": [
    "# similar to a SQL window function, we want to join each user with itself in the previous month. We first sort data based on key columns...\n",
    "traindat = traindat.sort_values(['ncodpers','fecha_dato'],ascending=[True,True]).reset_index(drop=True)\n",
    "print('sort completed')\n",
    "\n",
    "# ...then create a new dataset where the index is incremented...\n",
    "traindat['new'] = traindat.index\n",
    "train_index = traindat.copy()\n",
    "train_index['new'] += 1\n",
    "\n",
    "# ...then merge the dataset with itself to add each user's purchases in the previous month (there is definitely a quicker way of doing this - I am still relatively new to Python!)\n",
    "# we rename these new columns with a '_previous' suffix\n",
    "merge_drop_cols = demographic_cols.copy()\n",
    "merge_drop_cols.remove('ncodpers')\n",
    "#traindat_use = pd.merge(traindat,train_index.drop(merge_drop_cols,1), on=['new','ncodpers'],how='left',suffixes=['','_previous']) \n",
    "#en √©criture correcte, axis=1 doit √™tre un argument nomm√©, pas positionnel.\n",
    "#Correction :\n",
    "traindat_use = pd.merge(traindat,train_index.drop(merge_drop_cols,axis=1), on=['new','ncodpers'],how='left',suffixes=['','_previous'])\n",
    "\n",
    "print('merge completed')\n",
    "\n",
    "# replace current with (current - previous) to obtain what we want: purchase indicators\n",
    "for i in product_col:\n",
    "    traindat_use[i] = traindat_use[i]-traindat_use[i+\"_previous\"]\n",
    "    # replace negative values with 0: if a user gets rid of a product from month x to month x+1, this registers as no purchase in the evaluation metric, so we also treat it as no purchase made\n",
    "    #traindat_use[i][traindat_use[i] < 0] = 0\n",
    "    #pour √©viter le warning (A value is trying to be set on a copy of a slice from a DataFrame)\n",
    "    #et garantir que la modification directement traindat_use\n",
    "    #Correction : \n",
    "    traindat_use.loc[traindat_use[i] < 0, i] = 0\n",
    "\n",
    "# fill in na values created by merge\n",
    "traindat_use[product_col] = traindat_use[product_col].fillna(0)\n",
    "new_product_col = [i + \"_previous\" for i in product_col]\n",
    "traindat_use[new_product_col] = traindat_use[new_product_col].fillna(0)\n",
    "\n",
    "# delete redundant objects to free up memory\n",
    "del train_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "9exKvaz6SNDD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2363"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We also want to add purchase history columns to the test data set, for the purposes of making predictions\n",
    "\n",
    "test_col = product_col + ['ncodpers']\n",
    "testdat_use = pd.merge(testdat,traindat[traindat.fecha_dato=='2016-05-28'][test_col],on='ncodpers',how='left',suffixes=['','_previous'])\n",
    "\n",
    "testdat_use.rename(\n",
    "    columns={i:j for i,j in zip(product_col,new_product_col)}, inplace=True\n",
    ")\n",
    "\n",
    "testdat_use[new_product_col] = testdat_use[new_product_col].fillna(0)\n",
    "\n",
    "# delete redundant objects to free up memory\n",
    "del traindat, testdat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5DRlc64SNDE"
   },
   "source": [
    "## Step 4: De-duplicate the Data\n",
    "Building a similarity matrix for all test users takes a very long time and repeats a lot of work. Instead, we work out the similarity of each ** unique ** test user with all the training users, speeding up computation time enormously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "yNNqvY3eSNDE"
   },
   "outputs": [],
   "source": [
    "# pull through variables for memory-based CF\n",
    "\n",
    "traindat_purchases = traindat_use[traindat_use.fecha_dato == '2015-06-28'][product_col].copy()\n",
    "traindat_final = traindat_use[traindat_use.fecha_dato == '2015-06-28'][new_product_col].copy()\n",
    "\n",
    "# pull through variables for demographic-based CF\n",
    "\n",
    "demog_col = ['sexo','age','fecha_alta','ind_nuevo','indrel','indresi','indfall','tipodom','ind_actividad_cliente']\n",
    "traindat_demog_final = traindat_use[traindat_use.fecha_dato == '2015-06-28'][demog_col].copy()\n",
    "\n",
    "# transform demographic factor variables into binary format\n",
    "\n",
    "sexo_map = {'V': 1,'H': 0}\n",
    "age_map = {'old': 1,'young': 0}\n",
    "fecha_alta_map = {'(1994-12-31, 2010-12-31]': 1,'(2010-12-31, 2026-12-31]': 0}\n",
    "indresi_map = {'S': 1,'N': 0}\n",
    "indfall_map = {'S': 1,'N': 0}\n",
    "\n",
    "traindat_demog_final.loc[traindat_demog_final['age']=='older', 'age'] = 'old'\n",
    "traindat_demog_final.loc[traindat_demog_final['age']=='middle', 'age'] = 'young'\n",
    "traindat_demog_final.sexo = [sexo_map[item] for item in traindat_demog_final.sexo]\n",
    "traindat_demog_final.age = [age_map[item] for item in traindat_demog_final.age]\n",
    "traindat_demog_final.fecha_alta = [fecha_alta_map[item] for item in traindat_demog_final.fecha_alta]\n",
    "traindat_demog_final.indresi = [indresi_map[item] for item in traindat_demog_final.indresi]\n",
    "traindat_demog_final.indfall = [indfall_map[item] for item in traindat_demog_final.indfall]\n",
    "\n",
    "# we want all the observed combinations of purchase history\n",
    "new_product_col_aug = new_product_col + ['ncodpers']\n",
    "testdat_final = testdat_use[new_product_col_aug].copy()\n",
    "#testdat_final_unique = testdat_final.drop('ncodpers',1).drop_duplicates().copy().reset_index(drop=True)\n",
    "#Correction: \n",
    "testdat_final_unique = testdat_final.drop('ncodpers',axis=1).drop_duplicates().copy().reset_index(drop=True)\n",
    "#testdat_final_unique.shape\n",
    "# 6510 unique combinations of purchase history\n",
    "\n",
    "# transform demographic factor variables into binary format\n",
    "\n",
    "demog_col_aug = demog_col + ['ncodpers']\n",
    "testdat_demog_final = testdat_use[demog_col_aug].copy()\n",
    "\n",
    "testdat_demog_final.loc[testdat_demog_final['age']=='older', 'age'] = 'old'\n",
    "testdat_demog_final.loc[testdat_demog_final['age']=='middle', 'age'] = 'young'\n",
    "testdat_demog_final.sexo = [sexo_map[item] for item in testdat_demog_final.sexo]\n",
    "testdat_demog_final.age = [age_map[item] for item in testdat_demog_final.age]\n",
    "testdat_demog_final.fecha_alta = [fecha_alta_map[item] for item in testdat_demog_final.fecha_alta]\n",
    "testdat_demog_final.indresi = [indresi_map[item] for item in testdat_demog_final.indresi]\n",
    "testdat_demog_final.indfall = [indfall_map[item] for item in testdat_demog_final.indfall]\n",
    "#meme correction ici:\n",
    "testdat_demog_final_unique = testdat_demog_final.drop('ncodpers',axis=1).drop_duplicates().copy().reset_index(drop=True)\n",
    "\n",
    "#testdat_demog_final_unique.shape\n",
    "#114 unique combinations of demographics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agyJZsGRSNDG"
   },
   "source": [
    "We now have everything we need to perform memory-based and demographic-based CF. If we want to run either of these models, we can use all the relevant data to get the strongest possible submission. However, if we want to combine these models, we need to use an evaluation metric to ascertain optimal combination weights. To do this, we can train a first-stage model using 80% of the training data as our 'training' and 20% as our 'test' data, find optimal weights, and run the final model on all the data using these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "61gQb8UrSNDH"
   },
   "outputs": [],
   "source": [
    "# Split the training data into 'training' and 'test' sets\n",
    "\n",
    "# create 80% index\n",
    "traindat_index = np.random.rand(len(traindat_final)) < 0.8\n",
    "# create traindat_train\n",
    "traindat_train = traindat_final[traindat_index]\n",
    "# create traindat_test\n",
    "traindat_test = traindat_final[~traindat_index]\n",
    "# make traindat_test unique\n",
    "traindat_test_unique = traindat_test.drop_duplicates().copy().reset_index(drop=True)\n",
    "# create traindat_purchases\n",
    "traindat_purchases_train = traindat_purchases[traindat_index]\n",
    "# create traindat_purchases_test for verification\n",
    "traindat_purchases_test = traindat_purchases[~traindat_index]\n",
    "# create training ncodpers index\n",
    "traindat_ncodpers = traindat_use[traindat_use.fecha_dato == '2015-06-28'][traindat_index][['fecha_dato','ncodpers']]\n",
    "traindat_test_ncodpers = traindat_use[traindat_use.fecha_dato == '2015-06-28'][~traindat_index][['fecha_dato','ncodpers']]\n",
    "\n",
    "# repeat for demographic columns\n",
    "# create traindat_demog_train\n",
    "traindat_demog_train = traindat_demog_final[traindat_index]\n",
    "# create traindat_demog_test\n",
    "traindat_demog_test = traindat_demog_final[~traindat_index]\n",
    "# make traindat_demog_test unique\n",
    "traindat_demog_test_unique = traindat_demog_test.drop_duplicates().copy().reset_index(drop=True)\n",
    "# purchase indices are the same as for memory-based model data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apDoozA9SNDH"
   },
   "source": [
    "## Step 5: Build Demographic-Based and Memory-Based Similarity Matrices\n",
    "A similarity matrix, even on a few thousand unique data points, would be vast, so we iterated over each of the unique test profiles. The steps of the algorithm should be clear in the below code, but at a high level we calculated distances to each training point, weighted training purchases by the inverse distances to obtain a purchase probability, and nullified the purchase probability of owned items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "m_VNiZl7SNDH"
   },
   "outputs": [],
   "source": [
    "def probability_calculation(dataset, training, training_purchases, used_columns, metric, test_remap, print_option=False):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import metrics\n",
    "\n",
    "    n = dataset.shape[0]\n",
    "    probabilities = []\n",
    "\n",
    "    for index, row in dataset.iterrows():\n",
    "        #DataFrame.append() a √©t√© supprim√© dans pandas 2.0.\n",
    "        #Vous devez remplacer son usage par pd.concat(), qui est maintenant la m√©thode recommand√©e pour empiler des DataFrames.\n",
    "        #Correction : \n",
    "        if print_option:\n",
    "            print(f\"{index}/{n}\")\n",
    "        row_use = row.to_frame().T\n",
    "        distances = metrics.pairwise_distances(row_use, training, metric=metric) + 1e-6\n",
    "        norm_distances = 1 / distances\n",
    "        sim = pd.DataFrame(\n",
    "            norm_distances.dot(training_purchases) / np.sum(norm_distances),\n",
    "            columns=new_product_col\n",
    "        )\n",
    "        probabilities.append(sim)\n",
    "\n",
    "    print(\"probabilities calculated\")\n",
    "\n",
    "    # üõ†Ô∏è Concat√©ner la liste de DataFrames\n",
    "    probabilities = pd.concat(probabilities, ignore_index=True)\n",
    "\n",
    "    # üîÅ Reindex pour fusion propre avec dataset/test_remap\n",
    "    reindexed_output = probabilities.reset_index(drop=True).copy()\n",
    "    indexed_unique_test = dataset.reset_index(drop=True).copy()\n",
    "\n",
    "    output_unique = indexed_unique_test.join(reindexed_output, rsuffix='_predict')\n",
    "    output_final = pd.merge(test_remap, output_unique, on=used_columns, how='left')\n",
    "\n",
    "    # Ne garder que les colonnes produits (pas les colonnes de features utilis√©es)\n",
    "    output_final = output_final.drop(columns=used_columns)\n",
    "\n",
    "    # Nettoyage des noms de colonnes\n",
    "    output_final.columns = output_final.columns.str.replace(\"_predict\", \"\", regex=False)\n",
    "    output_final.columns = output_final.columns.str.replace(\"_previous\", \"_predict\", regex=False)\n",
    "\n",
    "    return output_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ2BWCJHSNDI"
   },
   "source": [
    "## Step 6: Combine Demographic-Based and Memory-Based Probabilities\n",
    "Using the probabilities derived in step 5, we can derive a weighted average (for 5 candidate weights, ranging from 0-1) to incorporate both data sources into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "vRtW-yzrSNDI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities calculated\n",
      "probabilities calculated\n"
     ]
    }
   ],
   "source": [
    "# calculate memory-based similarities\n",
    "probabilities_memory = probability_calculation(traindat_test_unique,traindat_train,traindat_purchases_train,new_product_col,'manhattan',traindat_test)\n",
    "# dans la fonction probability_calculation, or .append() n‚Äôexiste plus dans pandas ‚â• 2.0.\n",
    "# Correction : \n",
    "# calculate demographic-based similarities\n",
    "probabilities_demog = probability_calculation(traindat_demog_test_unique,traindat_demog_train,traindat_purchases_train,demog_col,'manhattan',traindat_demog_test)\n",
    "# average predictions for a range of mixing probabilities\n",
    "probabilities_avg_90 = 0.9*probabilities_memory + 0.1*probabilities_demog\n",
    "probabilities_avg_70 = 0.7*probabilities_memory + 0.3*probabilities_demog\n",
    "probabilities_avg_50 = 0.5*probabilities_memory + 0.5*probabilities_demog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7bhpUi6SNDI"
   },
   "source": [
    "Now that we have probabilities, we want to remove the possibility of predicting a product that a user already owns. Nullifying previous purchases can be done at step 5, but we do it here to avoid repeating work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "_uE0d1CuSNDJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def purchase_nullifier(probabilities, purchase_history, print_option=False):\n",
    "    # fonction qui \"annule\" les pr√©dictions pour des produits d√©j√† poss√©d√©s\n",
    "    purchase_history = purchase_history.reset_index(drop=True)\n",
    "    joined_data = purchase_history.join(probabilities)\n",
    "    unique_data = joined_data.drop_duplicates().copy().reset_index(drop=True)\n",
    "    n = unique_data.shape[0]\n",
    "    print(\"data joined\")\n",
    "    \n",
    "    # On stocke les lignes dans une liste au lieu d‚Äôutiliser .append()\n",
    "    output_norm_list = []\n",
    "\n",
    "    for index, row in unique_data.iterrows():\n",
    "        #Meme erreur que la cellule precedente\n",
    "        #Correction:\n",
    "        if print_option:\n",
    "            print(str(index) + \"/\" + str(n))\n",
    "        row = row.to_frame().T\n",
    "        row_purchases = row[new_product_col]\n",
    "        row_purchases.columns = row_purchases.columns.str.replace(\"_previous\", \"\")\n",
    "        row_probabilities = row[predict_col]\n",
    "        row_probabilities.columns = row_probabilities.columns.str.replace(\"_predict\", \"\")\n",
    "        \n",
    "        prob_norm = (1 - row_purchases).multiply(row_probabilities, axis=0)\n",
    "        output_norm_list.append(prob_norm)\n",
    "\n",
    "    # Concat√©nation de tous les r√©sultats\n",
    "    output_norm = pd.concat(output_norm_list, ignore_index=True)\n",
    "\n",
    "    print(\"nullification complete\")\n",
    "    \n",
    "    output_index = output_norm.reset_index(drop=True)\n",
    "    prob_predict = output_index.join(unique_data)\n",
    "    scaled_predict = pd.merge(joined_data, prob_predict, how='left')\n",
    "    \n",
    "    output = scaled_predict[product_col]\n",
    "    output.columns = output.columns.str.replace(\"ult1\", \"ult1_predict\")\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "rAZiwyz-SNDJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n",
      "data joined\n",
      "nullification complete\n"
     ]
    }
   ],
   "source": [
    "nulled_probabilities_100 = purchase_nullifier(probabilities_memory,traindat_test)\n",
    "nulled_probabilities_90 = purchase_nullifier(probabilities_avg_90,traindat_test)\n",
    "nulled_probabilities_70 = purchase_nullifier(probabilities_avg_70,traindat_test)\n",
    "nulled_probabilities_50 = purchase_nullifier(probabilities_avg_50,traindat_test)\n",
    "nulled_probabilities_0 = purchase_nullifier(probabilities_demog,traindat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwOAU7ZjSNDJ"
   },
   "source": [
    "## Step 7: Derive Recommendations\n",
    "Keeping purchase probabilities rather than simply outputting the recommendations is very useful for further analysis, e.g. model averaging. In this step we use the probabilities to derive recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "C5paFdMBSNDK"
   },
   "outputs": [],
   "source": [
    "#! erreur : L'erreur survient parce que row_use.columns.to_series() retourne une Series unidimensionnelle, \n",
    "#tandis que vous tentez de l‚Äôindexer avec un tableau 2D issu de arank.values[:, ::-1][:, :7]. \n",
    "#Or, une pandas.Series ne peut √™tre index√©e qu‚Äôavec un index 1D, ce qui provoque une incompatibilit√© de dimensions et donc l‚Äôerreur.\n",
    "\n",
    "# Correction:\n",
    "#Le code a √©t√© corrig√© en rempla√ßant l‚Äôindexation 2D incorrecte sur une `Series` par une s√©lection directe des 7 colonnes avec les probabilit√©s les plus √©lev√©es via `sort_values()`. \n",
    "#La concat√©nation des recommandations a √©t√© simplifi√©e en une seule ligne. La gestion du suffixe `_predict` a √©t√© int√©gr√©e proprement avec `str.replace`. Enfin, la fusion finale a √©t√© ajust√©e pour conserver l‚Äôordre initial des `ncodpers`.\n",
    "\n",
    "def probabilities_to_predictions(probabilities, ncodpers, print_option=False):\n",
    "\n",
    "    # Supprimer les doublons pour acc√©l√©rer le calcul\n",
    "    unique_probabilities = probabilities.drop_duplicates().copy().reset_index(drop=True)\n",
    "    if print_option:\n",
    "        print(f\"Nombre de lignes uniques : {unique_probabilities.shape[0]}\")\n",
    "\n",
    "    recoms_list = []\n",
    "    n = unique_probabilities.shape[0]\n",
    "\n",
    "    # It√©rer sur chaque ligne de probabilit√©s\n",
    "    for index, row in unique_probabilities.iterrows():\n",
    "        if print_option:\n",
    "            print(f\"{index + 1}/{n}\")\n",
    "\n",
    "        # Trier les colonnes selon les probabilit√©s d√©croissantes\n",
    "        top7_cols = row.sort_values(ascending=False).index[:7]\n",
    "        \n",
    "        # Nettoyer les noms de colonnes si elles contiennent '_predict'\n",
    "        top7_cleaned = [col.replace('_predict', '') for col in top7_cols]\n",
    "        \n",
    "        # Concat√©ner les 7 noms en une seule cha√Æne\n",
    "        recom_str = ' '.join(top7_cleaned)\n",
    "        recoms_list.append(recom_str)\n",
    "\n",
    "    # Convertir la liste des recommandations en Series\n",
    "    predictions = pd.Series(recoms_list)\n",
    "\n",
    "    # Associer chaque ligne de prediction avec les probabilit√©s uniques\n",
    "    mapped_predictions = predictions.to_frame().rename(columns={0: 'added_products'}).reset_index(drop=True)\n",
    "    output_unique = mapped_predictions.join(unique_probabilities)\n",
    "\n",
    "    # Fusionner avec les probabilit√©s originales pour retrouver l'ordre\n",
    "    output_final = pd.merge(probabilities, output_unique, how='left', on=list(probabilities.columns))\n",
    "\n",
    "    # Joindre avec ncodpers pour avoir le r√©sultat final\n",
    "    ncodpers_reset = ncodpers.reset_index(drop=True)\n",
    "    output_ncodpers = ncodpers_reset.join(output_final['added_products'])\n",
    "\n",
    "    # Supprimer la colonne 'fecha_dato' si pr√©sente\n",
    "    if 'fecha_dato' in output_ncodpers.columns:\n",
    "        output_ncodpers = output_ncodpers.drop(columns=['fecha_dato'])\n",
    "\n",
    "    return output_ncodpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "EFG7r62lSNDK"
   },
   "outputs": [],
   "source": [
    "predictions_output_100 = probabilities_to_predictions(nulled_probabilities_100,traindat_test_ncodpers)\n",
    "predictions_output_90 = probabilities_to_predictions(nulled_probabilities_90,traindat_test_ncodpers)\n",
    "predictions_output_70 = probabilities_to_predictions(nulled_probabilities_70,traindat_test_ncodpers)\n",
    "predictions_output_50 = probabilities_to_predictions(nulled_probabilities_50,traindat_test_ncodpers)\n",
    "predictions_output_0 = probabilities_to_predictions(nulled_probabilities_0,traindat_test_ncodpers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cC73RdkhSNDK"
   },
   "source": [
    "The final piece of the puzzle is an evaluation metric, to ascertain the performance of each model (i.e. each mixing probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "qC8oNkqdSNDK"
   },
   "outputs": [],
   "source": [
    "evaluation_col = product_col + ['added_products']\n",
    "#erreur : DataFrame.drop() takes from 1 to 2 positional arguments but 3 were given\n",
    "def evaluation_metric(predictions,reality,print_option=False):\n",
    "    # predictions is a list of the top seven purchase likelihood indicators; reality is the actual purchases\n",
    "    reality = reality.reset_index(drop=True)\n",
    "    # find unique combinations to speed up function: merge data, group_by, count (then multiply results at the end)\n",
    "    reality['added_products'] = predictions['added_products']\n",
    "    data_unique = reality.drop_duplicates().copy().reset_index(drop=True)\n",
    "    predictions_unique = data_unique['added_products'].to_frame()\n",
    "    #Correction : \n",
    "    reality_unique = data_unique.drop('added_products',axis=1)\n",
    "    n = predictions_unique.shape[0]\n",
    "    for index, row in predictions_unique.iterrows():\n",
    "        if print_option == True:\n",
    "            print(str(index) + '/' + str(n))\n",
    "        prediction_use = row.to_frame().T['added_products'].str.split(' ',expand=True).T\n",
    "        prediction_use = prediction_use.rename(columns={list(prediction_use)[0]:'predict_products'})\n",
    "        #print(prediction_use)\n",
    "        # only take top 7 products purchased\n",
    "        reality_use = reality_unique.iloc[index].to_frame()\n",
    "        reality_use = reality_use.rename(columns={list(reality_use)[0]:'added_products'})\n",
    "        reality_use['product_name'] = reality_use.index\n",
    "        reality_use = reality_use[reality_use.added_products==1]\n",
    "        reality_use['ind'] = 1\n",
    "        #print(reality_use)\n",
    "        if reality_use.empty:\n",
    "            P = [0]\n",
    "        else:\n",
    "            # calculate precision @7: what average proportion of our predictions are purchased?\n",
    "            P = [precision_at_k(prediction_use,reality_use)]\n",
    "        if index == 0:\n",
    "            eval_sum = P\n",
    "        else:\n",
    "            eval_sum.extend(P)\n",
    "    # duplicate back up\n",
    "    print('precisions calculated')\n",
    "    data_unique['precision'] = eval_sum\n",
    "    reality_final = pd.merge(reality,data_unique,on=evaluation_col,how='left')\n",
    "    U = predictions.shape[0]\n",
    "    output = sum(reality_final.precision)/U\n",
    "    return output\n",
    "\n",
    "def precision_at_k(prediction,reality):\n",
    "    # 'prediction' is a data frame with a column 'predict_products' containing our 7 predictions\n",
    "    # 'reality' is a data frame with a column 'added_products' containing any products purchased (always non-empty)\n",
    "    summand = min(prediction.shape[0],7)\n",
    "    sum_prec = 0\n",
    "    for k in range(summand):\n",
    "        # for each k, calculate precision at k (careful with 0 index)\n",
    "        top_k_predictions = prediction.head(k+1)\n",
    "        # join additions to reduced predictions\n",
    "        add_vs_pred = pd.merge(reality,top_k_predictions,left_on='product_name',right_on='predict_products',how='inner')\n",
    "        sum_prec = sum_prec + sum(add_vs_pred.ind)/top_k_predictions.shape[0]\n",
    "    denom = min(reality.shape[0],7)\n",
    "    # always defined as in evaluation_metric function 'reality_use' is always non-empty\n",
    "    output = sum_prec/denom\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "7qacxNiQSNDM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n",
      "precisions calculated\n"
     ]
    }
   ],
   "source": [
    "evaluation_100 = evaluation_metric(predictions_output_100,traindat_purchases_test)\n",
    "evaluation_90 = evaluation_metric(predictions_output_90,traindat_purchases_test)\n",
    "evaluation_70 = evaluation_metric(predictions_output_70,traindat_purchases_test)\n",
    "evaluation_50 = evaluation_metric(predictions_output_50,traindat_purchases_test)\n",
    "evaluation_0 = evaluation_metric(predictions_output_0,traindat_purchases_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "x34BXoVWSNDN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all memory: 0.09874082970210296\n",
      "90% memory: 0.09913794055323125\n",
      "70% memory: 0.0986340650875955\n",
      "50% memory: 0.09801691288511784\n",
      "all demographics: 0.08509216158796049\n"
     ]
    }
   ],
   "source": [
    "print(\"all memory: \" + str(evaluation_100) + '\\n' +\n",
    "      \"90% memory: \" + str(evaluation_90) + '\\n' +\n",
    "      \"70% memory: \" + str(evaluation_70) + '\\n' +\n",
    "      \"50% memory: \" + str(evaluation_50) + '\\n' +\n",
    "      \"all demographics: \" + str(evaluation_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtImTYYASNDN"
   },
   "source": [
    "## Step 8: Re-run model using all training data for optimal mixing parameter\n",
    "90% memory gives the strongest results: the optimum seems to lie somewhere around 85%. Now that we have an optimal mixing probability, we can iterate through the entire process, using all of the training data to return our optimal prediction that leverages both demographic and purchase data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "vqlmbv2oSNDN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities calculated\n",
      "probabilities calculated\n",
      "data joined\n",
      "nullification complete\n"
     ]
    }
   ],
   "source": [
    "# calculate probabilities\n",
    "probability_85_memory = probability_calculation(testdat_final_unique,traindat_final,traindat_purchases,new_product_col,'manhattan',testdat_final)\n",
    "probability_85_demog = probability_calculation(testdat_demog_final_unique,traindat_demog_final,traindat_purchases,demog_col,'manhattan',testdat_demog_final)\n",
    "\n",
    "# average probabilities\n",
    "probability_avg_85 = 0.85*probability_85_memory + 0.15*probability_85_demog\n",
    "\n",
    "# write csv of averaged probabilities\n",
    "probability_avg_85.to_csv(\"probabilities_85_avg.csv\",index=False)\n",
    "\n",
    "# null previous ownership\n",
    "nulled_probability_85 = purchase_nullifier(probability_avg_85[predict_col],testdat_final[new_product_col])\n",
    "\n",
    "# map to predictions - check dimensions\n",
    "testdat_ncodpers = testdat_use[['fecha_dato','ncodpers']]\n",
    "predictions_output_85 = probabilities_to_predictions(nulled_probability_85,testdat_ncodpers)\n",
    "\n",
    "# send predictions to csv\n",
    "predictions_output_85.to_csv('submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9UlRCA5SNDO"
   },
   "source": [
    "The submission in its current form has a private score of 0.02644 and a public score of 0.02619, which is just outside the top 50% of scores. Not bad with no model-based input! This compares to respective scores of 0.02633 and 0.02610 when no mixing probabilities are used (i.e. we only use memory-based cf); a small improvement, but significant in a Kaggle context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWXxCn0PSNDP"
   },
   "source": [
    "## Next Steps\n",
    "This notebook is very much intended as an introduction to recommendation systems. There are several lines of analysis that could be pursued to improve upon this model's score:\n",
    "* ** Test different distance metrics and demographic weightings: ** in our final algorithm we use a very straightforward distance metric, and weight purchases by taking the inverse sum of this straightforward metric. There are improvements to be made by sharpening up this methodology.\n",
    "* ** Include item-based CF and average results: ** the products are real, and we can derive product features manually. This could help us to build an item-based CF approach, which could be combined with our algorithmically-defined memory-based probabilities to enhance to model.\n",
    "* ** Try model-based CF: ** memory-based CF is the most straightforward technique, and it will be interesting to compare the performance of these models with the performance of out memory-based algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
